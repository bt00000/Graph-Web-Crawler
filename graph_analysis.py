import json
import networkx as nx
import matplotlib.pyplot as plt
from urllib.parse import urlparse
from collections import defaultdict

# Load graph data from JSON
with open("graph_data.json", "r") as f:
    page_graph = json.load(f)

# Create a Directed Graph
G = nx.DiGraph()

# Add nodes and edges
for page, links in page_graph.items():
    G.add_node(page)
    for link in links:
        G.add_edge(page, link)

# **Step 1: Degree Distribution Analysis**
in_degrees = [G.in_degree(n) for n in G.nodes()]
out_degrees = [G.out_degree(n) for n in G.nodes()]

print(f"\nüìä Graph Statistics:")
print(f"Total Pages: {len(G.nodes())}")
print(f"Total Links: {len(G.edges())}")
print(f"Max In-Degree: {max(in_degrees)}")
print(f"Min In-Degree: {min(in_degrees)}")
print(f"Average In-Degree: {sum(in_degrees) / len(in_degrees):.2f}")

# **Step 2: Improve PageRank Computation**
# **üîπ Remove Internal Navigation & Query Parameters**
internal_pages = {
    "https://news.ycombinator.com/submit",
    "https://news.ycombinator.com/jobs",
    "https://news.ycombinator.com/lists",
    "https://news.ycombinator.com/newest",
    "https://news.ycombinator.com/newcomments",
    "https://news.ycombinator.com/ask",
    "https://news.ycombinator.com/show",
    "https://news.ycombinator.com/from?site=ycombinator.com",
}

# **üîπ Remove Login, Hide, Favorite, and Other Query-Based URLs**
filtered_graph = G.copy()
for node in list(G.nodes()):
    if node in internal_pages or "?" in node:  # Removes query-based URLs
        filtered_graph.remove_node(node)

# **üîπ Remove Orphan Pages (Pages with No Incoming Links)**
orphan_nodes = [node for node in filtered_graph.nodes if filtered_graph.in_degree(node) == 0]
filtered_graph.remove_nodes_from(orphan_nodes)

# **üîπ Compute PageRank with Standard Damping Factor (0.85)**
pagerank = nx.pagerank(filtered_graph, alpha=0.85)

# **Sort Pages by PageRank Score**
top_pages = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)[:10]

# **Print Top 10 Important Pages**
print("\nüèÜ Top 10 Most Important Pages (Filtered PageRank):")
for rank, (page, score) in enumerate(top_pages, start=1):
    print(f"{rank}. {page} -> Score: {score:.5f}")

# **Step 3: Identify Connected Components**
strong_components = list(nx.strongly_connected_components(G))
weak_components = list(nx.weakly_connected_components(G))

print(f"\nüîó Total Strongly Connected Components: {len(strong_components)}")
print(f"üîó Total Weakly Connected Components: {len(weak_components)}")

# **Find Largest Weakly Connected Component**
largest_weak_component = max(weak_components, key=len)
print(f"\nüî• Largest Weakly Connected Component has {len(largest_weak_component)} pages.")

# **Step 4: Find Most Influential Domains**
domain_pagerank = defaultdict(float)

for page, score in pagerank.items():
    domain = urlparse(page).netloc
    domain_pagerank[domain] += score

# **Sort Domains by Total PageRank Score**
top_domains = sorted(domain_pagerank.items(), key=lambda x: x[1], reverse=True)[:10]

# **Print Top Domains by Influence**
print("\nüåé Top 10 Most Influential Domains (Aggregated PageRank):")
for rank, (domain, score) in enumerate(top_domains, start=1):
    print(f"{rank}. {domain} -> Score: {score:.5f}")

# ** Step 5: Visualizations**

# **1. In-Degree Distribution**
plt.figure(figsize=(10, 5))
plt.hist(in_degrees, bins=50, color='blue', alpha=0.7)
plt.xlabel("In-Degree (Number of Incoming Links)")
plt.ylabel("Frequency")
plt.title("In-Degree Distribution of Web Pages")
plt.yscale("log")  # Log scale for better visibility
plt.show()

# **2. Out-Degree Distribution**
plt.figure(figsize=(10, 5))
plt.hist(out_degrees, bins=50, color='green', alpha=0.7)
plt.xlabel("Out-Degree (Number of Outgoing Links)")
plt.ylabel("Frequency")
plt.title("Out-Degree Distribution of Web Pages")
plt.yscale("log")  # Log scale for better visibility
plt.show()

# **3. PageRank Score Distribution**
pagerank_scores = list(pagerank.values())
plt.figure(figsize=(10, 5))
plt.hist(pagerank_scores, bins=50, color='purple', alpha=0.7)
plt.xlabel("PageRank Score")
plt.ylabel("Frequency")
plt.title("Distribution of PageRank Scores")
plt.yscale("log")  # Log scale for better visibility
plt.show()

# **4. Connected Component Size Distribution**
component_sizes = [len(comp) for comp in weak_components]
plt.figure(figsize=(10, 5))
plt.hist(component_sizes, bins=20, color='red', alpha=0.7)
plt.xlabel("Size of Weakly Connected Components")
plt.ylabel("Frequency")
plt.title("Distribution of Connected Component Sizes")
plt.yscale("log")
plt.show()

# **Step 6: Export to Gephi (GEXF Format)**
nx.write_gexf(G, "graph_data.gexf")
print("\nüìÅ Graph saved as 'graph_data.gexf'. Open it in Gephi for analysis.")
